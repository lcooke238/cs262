# COMPSCI 262 PSET 1: Overall notebook

## Introduction

This notebook gives the high level guidance to the whole repository. The repo is split into the gRPC and custom implementations. We have separate notebook files for the details of each version within their respective folders. Structure:

* `gRPC`

    -> <b>code</b>: stores the code files, including `server_grpc.py`, `client_grpc.py` and `test_grpc.py`. 

    -> <b>data</b>: stores the database for the server.
    
    -> <b>logs</b>: Stores logs generated by the code.
    
    -> <b>protos</b>: Stores the protos for the gRPC implementation.
    
    -> <b>`gRPC_ledger.md`</b>: The notebook/ ledger for the gRPC implementation.
    
    -> <b>`README.md`</b>: Instructions for installing the prerequisites and running the gRPC implementation.

* `custom_wire`
    
    -> <b>code</b>: stores the code files, including `server_custom.py`, `client_custom.py` and `server_tests.py`. 
    
    -> <b>data</b>: stores the data for the server.
    
    -> <b>logs</b>: Stores logs generated by the code.
    
    -> <b>`custom_wire_ledger.md`</b>: The notebook/ ledger for the custom wire protocol implementation.
    
    -> <b>`README.md`</b>: Instructions for installing the prerequisites and running the custom wire implementation.

## The two versions

We implemented two very different versions of a chat application (as permitted by the 'alter' in the Canvas assignment!), some of the main differences are:

1. For the gRPC implementation, both client and server use multithreading, whereas the custom wire implementation uses no multithreading at all. The server for the custom implementation used the select library to switch beyond socket connections to serve multiple clients on a single thread, whereas the gRPC server runs a thread for each client connected, and runs two threads on the client, one to listen for data from the server and one to send data. Some of the tradeoffs of the two implementations:

    -> For the gRPC version with multithreading, we have to fight data races in the sever by locking whenever modifying the data. The custom solution sidesteps this problem entirely by constantly switching between who it serves, but crucially only serving one client at a time in one thread, such that all data manipulation is much safer.

    -> The gRPC solution can pull unread messages instantly, since it has a listener thread always trying to pull messages. There's an obvious performance tradeoff of running a thread in a `while True` loop to constantly get messages, but it ensures any messages are pulled almost instantly. For a large number of clients, this might cause issues with the one lock being very hard to acquire, but for low client numbers it works well. The custom solution on the other hand pulls messages whenever a command is entered - if you hit return, whether alone or when using a command, it will pull unread messages. This means you don't receive messages instantly which is somewhat annoying, but has the benefit of not polling with another thread forever and also formats the messages more cleanly: in the gRPC version, it's very hard to interrupt the `input` loop of the main thread from the listener, so the formatting upon receiving a message is a little bit messy. By pulling the messages in between commands, the custom solution looks prettier when messages are received.

2. The data structure to store server information is very different for the two implementations. For the gRPC version, I used an SQL database, whereas in the custom version we used a CSV file to store the data.

    -> The tradeoffs here were a little more simple - I have used SQL to a reasonable extent whereas my partner has never used SQL, so decided to go with something they were more familiar with. The SQL database allows us to nicely search for users using SQL wildcard syntax, whereas the CSV file version is more restricted (without attempting to implement the same functionality, which would be a pain and way beyond the spec).

3. The length/ complexity of the two versions is very different: our gRPC implementation is significantly shorter than the custom version.

    -> This was certainly expected since gRPC does a lot of the work for transferring data between client and server for us. If you consider the length of the files gRPC generates for us, then the gRPC implementation is almost certainly larger, but just in terms of the code that we wrote, the gRPC solution is shorter and significantly less complicated.

    -> There's also the factor that one of us is a bit comment crazy, so that also affects the length of the code pretty significantly.

4. The size of the buffers being sent back and forth seem to be similar. According to the internet, gRPC also pads 4 bytes to the front to store the message size along with a compression flag and then the message immediately follows. The messages are capped at 4MB (4 bytes storing length can only store a length of up to 4MB) so also have a similar cap to our custom version.

## Custom Wire Protocol
The detailed description of how this wire protocol was developed can be found in the custom_wire folder in the <b>`custom_wire_ledger.md`</b> file. Here is a summary of the wire protocol:

1. first 4 bytes (constant) represent the wire protocol version number. Upon reciept, a server and client will always check that this version number is what it expects, otherwise it will log the problem on the recieving side and close the socket. 
    - I chose to close the socket after logging the problem locally because we have no means of properly communicating the error back to the sender with a faulty wire protocol. 
2. next 1 byte (constant) represents the operation code. The operations are as follows:
    - 0: send a message to another user
        - next 4 bytes (constant) represents the length of the rest of the input to recieve (i)
        - next 1 byte (constant) represents the length of the username of the recipient (u)
        - next u bytes represents the username of the recipient
        - next i-1-u bytes represent the message
    - 1: logout
        - next 4 bytes (constant) represents the length of the rest of the input to recieve
        - rest of input represents the confirmation keyword to ensure opcode reciept wasn't a mistake
    - 2: delete
        - next 4 bytes (constant) represents the length of the rest of the input to recieve
        - rest of input represents the confirmation keyword to ensure opcode reciept wasn't a mistake
    - 3: login
        - next 4 bytes (constant) represents the length of the rest of the input to recieve
        - rest of input represents the username for the login
    - 4: list accounts
        - next 4 bytes (constant) represents length of rest of input
        - rest of input represents the keyword to filter list by
    - 5: error/exception
        - next 4 bytes (constant) represents the length of the rest of the input to recieve
        - rest of input represents the error message to be used
